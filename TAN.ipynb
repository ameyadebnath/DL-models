{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOl8sptvqBzIfzKqjf7N2T6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9GJAA4sn-Bt","executionInfo":{"status":"ok","timestamp":1708434918182,"user_tz":-360,"elapsed":887741,"user":{"displayName":"Ameya Debnath","userId":"03238844752105508856"}},"outputId":"476ae633-675c-49ac-e560-51f81127e109"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","110/110 [==============================] - 92s 776ms/step - loss: 0.6181 - accuracy: 0.6890 - val_loss: 0.5670 - val_accuracy: 0.6690\n","Epoch 2/10\n","110/110 [==============================] - 84s 758ms/step - loss: 0.3111 - accuracy: 0.8780 - val_loss: 0.3092 - val_accuracy: 0.8832\n","Epoch 3/10\n","110/110 [==============================] - 89s 811ms/step - loss: 0.1028 - accuracy: 0.9662 - val_loss: 0.3164 - val_accuracy: 0.8900\n","Epoch 4/10\n","110/110 [==============================] - 83s 759ms/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.3959 - val_accuracy: 0.8935\n","Epoch 5/10\n","110/110 [==============================] - 81s 740ms/step - loss: 0.0259 - accuracy: 0.9928 - val_loss: 0.4462 - val_accuracy: 0.8969\n","Epoch 6/10\n","110/110 [==============================] - 80s 725ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 0.4915 - val_accuracy: 0.8981\n","Epoch 7/10\n","110/110 [==============================] - 83s 754ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.5251 - val_accuracy: 0.8877\n","Epoch 8/10\n","110/110 [==============================] - 81s 733ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 0.4554 - val_accuracy: 0.8958\n","Epoch 9/10\n","110/110 [==============================] - 82s 740ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.5641 - val_accuracy: 0.8923\n","Epoch 10/10\n","110/110 [==============================] - 80s 725ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.5666 - val_accuracy: 0.8958\n","35/35 [==============================] - 6s 153ms/step\n","35/35 [==============================] - 6s 174ms/step - loss: 0.4228 - accuracy: 0.9185\n","Test Loss: 0.4228, Test Accuracy: 91.85%\n","Target 1 Metrics:\n","  Accuracy: 85.54%\n","  Precision: 0.7586\n","  Recall: 0.8148\n","  F1 Score: 0.7857\n"," Confusion Matrix:\n","[[49  7]\n"," [ 5 22]]\n","\n","\n","Target 0 Metrics:\n","  Accuracy: 50.00%\n","  Precision: 0.0000\n","  Recall: 0.0000\n","  F1 Score: 0.0000\n"," Confusion Matrix:\n","[[1 0]\n"," [1 0]]\n","\n","\n","Target 5 Metrics:\n","  Accuracy: 90.00%\n","  Precision: 1.0000\n","  Recall: 0.7500\n","  F1 Score: 0.8571\n"," Confusion Matrix:\n","[[6 0]\n"," [1 3]]\n","\n","\n","Target 3 Metrics:\n","  Accuracy: 91.67%\n","  Precision: 0.9091\n","  Recall: 0.7895\n","  F1 Score: 0.8451\n"," Confusion Matrix:\n","[[91  3]\n"," [ 8 30]]\n","\n","\n","Target 4 Metrics:\n","  Accuracy: 100.00%\n","  Precision: 0.0000\n","  Recall: 0.0000\n","  F1 Score: 0.0000\n"," Confusion Matrix:\n","[[2]]\n","\n","\n","Target 2 Metrics:\n","  Accuracy: 66.67%\n","  Precision: 0.5000\n","  Recall: 1.0000\n","  F1 Score: 0.6667\n"," Confusion Matrix:\n","[[1 1]\n"," [0 1]]\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention, Concatenate\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","\n","# Load your dataset\n","df = pd.read_csv('/content/final_dataset - Sheet1.csv')\n","\n","# Split the data into training and testing sets\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Tokenize the comments\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n","tokenizer.fit_on_texts(train_df['Comment'])\n","\n","# Convert comments to sequences\n","train_sequences = tokenizer.texts_to_sequences(train_df['Comment'])\n","test_sequences = tokenizer.texts_to_sequences(test_df['Comment'])\n","\n","# Pad sequences\n","max_sequence_length = max(len(seq) for seq in train_sequences)\n","X_train = pad_sequences(train_sequences, maxlen=max_sequence_length)\n","X_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n","\n","# Prepare target data\n","y_train = np.array(train_df['Stance'])\n","y_test = np.array(test_df['Stance'])\n","\n","# Prepare target-specific information\n","target_info_train = np.array(train_df['Target'])\n","target_info_test = np.array(test_df['Target'])\n","\n","# Build TAN model\n","embedding_dim = 100\n","\n","# Input layers\n","input_comment = Input(shape=(max_sequence_length,), name='input_comment')\n","input_target = Input(shape=(1,), name='input_target')\n","\n","# Embedding layer for comments\n","embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(input_comment)\n","\n","# Bidirectional LSTM layer\n","lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n","\n","# Attention layer\n","# attention_layer = Attention()([lstm_layer, tf.expand_dims(input_target, axis=-1)])\n","attention_layer = Attention()([lstm_layer, tf.tile(tf.expand_dims(input_target, axis=-1), multiples=[1, 1, 128])])\n","\n","\n","# Concatenate attention output with target information\n","merged_layer = Concatenate()([lstm_layer, attention_layer])\n","\n","# GlobalMaxPooling1D layer\n","global_max_pooling_layer = GlobalMaxPooling1D()(merged_layer)\n","\n","# Dense layers\n","dense_layer = Dense(32, activation='relu')(global_max_pooling_layer)\n","output_layer = Dense(1, activation='sigmoid', name='output')(dense_layer)\n","\n","# Build and compile the model\n","model = Model(inputs=[input_comment, input_target], outputs=output_layer)\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","#\n","# Train the model\n","model.fit([X_train, target_info_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n","\n","\n","# Evaluate the model on the test set\n","y_pred = model.predict([X_test, target_info_test])\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","# Overall metrics\n","test_loss, test_accuracy = model.evaluate([X_test, target_info_test], y_test)\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n","\n","# Calculate metrics for each target class\n","unique_targets = df['Target'].unique()\n","\n","for target_class in unique_targets:\n","    indices = test_df[test_df['Target'] == target_class].index\n","    indices = indices[indices < len(y_test)]  # Ensure indices are within bounds\n","\n","    y_test_target = y_test[indices]\n","    y_pred_target = y_pred_binary[indices]\n","\n","    accuracy = accuracy_score(y_test_target, y_pred_target)\n","    precision = precision_score(y_test_target, y_pred_target)\n","    recall = recall_score(y_test_target, y_pred_target)\n","    f1 = f1_score(y_test_target, y_pred_target)\n","    cm = confusion_matrix(y_test_target, y_pred_target)\n","\n","    print(f'Target {target_class} Metrics:')\n","    print(f'  Accuracy: {accuracy * 100:.2f}%')\n","    print(f'  Precision: {precision:.4f}')\n","    print(f'  Recall: {recall:.4f}')\n","    print(f'  F1 Score: {f1:.4f}')\n","    print(f' Confusion Matrix:\\n{cm}')\n","    print('\\n')"]}]}