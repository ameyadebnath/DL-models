{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYKlIbB+Fj0PLa9fPj8wux"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwQZ1NPh4bHu","executionInfo":{"status":"ok","timestamp":1708405952730,"user_tz":-360,"elapsed":333168,"user":{"displayName":"Ameya Debnath","userId":"03238844752105508856"}},"outputId":"84d27292-2d79-4130-c71a-d1b8bff58262"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","110/110 [==============================] - 36s 279ms/step - loss: 0.5734 - accuracy: 0.6896 - val_loss: 0.5125 - val_accuracy: 0.7079\n","Epoch 2/10\n","110/110 [==============================] - 28s 255ms/step - loss: 0.4671 - accuracy: 0.7440 - val_loss: 0.3942 - val_accuracy: 0.8362\n","Epoch 3/10\n","110/110 [==============================] - 28s 256ms/step - loss: 0.2081 - accuracy: 0.9192 - val_loss: 0.3002 - val_accuracy: 0.8877\n","Epoch 4/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0612 - accuracy: 0.9811 - val_loss: 0.3026 - val_accuracy: 0.9015\n","Epoch 5/10\n","110/110 [==============================] - 28s 253ms/step - loss: 0.0179 - accuracy: 0.9966 - val_loss: 0.3439 - val_accuracy: 0.9038\n","Epoch 6/10\n","110/110 [==============================] - 28s 255ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.3742 - val_accuracy: 0.9003\n","Epoch 7/10\n","110/110 [==============================] - 28s 258ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.3910 - val_accuracy: 0.8992\n","Epoch 8/10\n","110/110 [==============================] - 28s 256ms/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.4051 - val_accuracy: 0.9003\n","Epoch 9/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.4047 - val_accuracy: 0.9038\n","Epoch 10/10\n","110/110 [==============================] - 28s 251ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.4202 - val_accuracy: 0.9003\n","35/35 [==============================] - 2s 53ms/step\n","35/35 [==============================] - 2s 53ms/step - loss: 0.3091 - accuracy: 0.9267\n","Test Loss: 0.3091, Test Accuracy: 92.67%\n","Target 1 Metrics:\n","  Accuracy: 89.16%\n","  Precision: 0.8214\n","  Recall: 0.8519\n","  F1 Score: 0.8364\n","Target 0 Metrics:\n","  Accuracy: 50.00%\n","  Precision: 0.0000\n","  Recall: 0.0000\n","  F1 Score: 0.0000\n","Target 5 Metrics:\n","  Accuracy: 100.00%\n","  Precision: 1.0000\n","  Recall: 1.0000\n","  F1 Score: 1.0000\n","Target 3 Metrics:\n","  Accuracy: 93.94%\n","  Precision: 0.9167\n","  Recall: 0.8684\n","  F1 Score: 0.8919\n","Target 4 Metrics:\n","  Accuracy: 100.00%\n","  Precision: 0.0000\n","  Recall: 0.0000\n","  F1 Score: 0.0000\n","Target 2 Metrics:\n","  Accuracy: 66.67%\n","  Precision: 0.5000\n","  Recall: 1.0000\n","  F1 Score: 0.6667\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]}],"source":["#Gated Convolutional Network with aspect Embedding (based on a CNN model)\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Multiply, Flatten\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load your dataset\n","df = pd.read_csv('/content/final_dataset - Sheet1.csv')\n","\n","# Split the data into training and testing sets\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Tokenize the comments\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n","tokenizer.fit_on_texts(train_df['Comment'])\n","\n","# Convert comments to sequences\n","train_sequences = tokenizer.texts_to_sequences(train_df['Comment'])\n","test_sequences = tokenizer.texts_to_sequences(test_df['Comment'])\n","\n","# Pad sequences\n","max_sequence_length = max(len(seq) for seq in train_sequences)\n","X_train = pad_sequences(train_sequences, maxlen=max_sequence_length)\n","X_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n","\n","# Prepare target data\n","y_train = np.array(train_df['Stance'])\n","y_test = np.array(test_df['Stance'])\n","\n","# Prepare target-specific information\n","target_info_train = np.array(train_df['Target'])\n","target_info_test = np.array(test_df['Target'])\n","\n","# Build GCAE model\n","embedding_dim = 100\n","aspect_embedding_dim = 20\n","num_filters = 64\n","filter_sizes = [3, 4, 5]\n","\n","# Input layers\n","input_comment = Input(shape=(max_sequence_length,), name='input_comment')\n","input_target = Input(shape=(1,), name='input_target')\n","\n","# Embedding layer for comments\n","embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(input_comment)\n","\n","# Convolutional layers\n","conv_layers = []\n","for filter_size in filter_sizes:\n","    conv_layer = Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)\n","    pooled_layer = GlobalMaxPooling1D()(conv_layer)\n","    conv_layers.append(pooled_layer)\n","\n","# Concatenate pooled convolutional layers\n","concatenated_layer = Concatenate()(conv_layers)\n","\n","# Flatten the concatenated layer\n","flattened_layer = Flatten()(concatenated_layer)\n","\n","# Aspect Embedding\n","aspect_embedding_layer = Embedding(input_dim=max_words, output_dim=aspect_embedding_dim)(input_target)\n","\n","# Flatten the aspect embedding layer\n","flattened_aspect_embedding = Flatten()(aspect_embedding_layer)\n","\n","# Ensure both flattened layers have the same number of dimensions\n","flattened_aspect_embedding = Dense(192)(flattened_aspect_embedding)\n","\n","# Multiply flattened aspect embedding with flattened concatenated layer\n","gated_layer = Multiply()([flattened_layer, flattened_aspect_embedding])\n","\n","# Dense layers\n","dense_layer = Dense(32, activation='relu')(gated_layer)\n","output_layer = Dense(1, activation='sigmoid', name='output')(dense_layer)\n","\n","# Build and compile the model\n","model = Model(inputs=[input_comment, input_target], outputs=output_layer)\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit([X_train, target_info_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n","\n","# Evaluate the model on the test set\n","y_pred = model.predict([X_test, target_info_test])\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","# Overall metrics\n","test_loss, test_accuracy = model.evaluate([X_test, target_info_test], y_test)\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n","\n","# Calculate metrics for each target class\n","unique_targets = df['Target'].unique()\n","\n","for target_class in unique_targets:\n","    indices = test_df[test_df['Target'] == target_class].index\n","    indices = indices[indices < len(y_test)]  # Ensure indices are within bounds\n","\n","    y_test_target = y_test[indices]\n","    y_pred_target = y_pred_binary[indices]\n","\n","    accuracy = accuracy_score(y_test_target, y_pred_target)\n","    precision = precision_score(y_test_target, y_pred_target)\n","    recall = recall_score(y_test_target, y_pred_target)\n","    f1 = f1_score(y_test_target, y_pred_target)\n","\n","    print(f'Target {target_class} Metrics:')\n","    print(f'  Accuracy: {accuracy * 100:.2f}%')\n","    print(f'  Precision: {precision:.4f}')\n","    print(f'  Recall: {recall:.4f}')\n","    print(f'  F1 Score: {f1:.4f}')"]}]}